{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information CNN - NIPS\n",
    "\n",
    "- Author: Quentin Boyadjian, ETS\n",
    "- Date: 15/02/20\n",
    "- Input : DTD texture database\n",
    "- Output : Mutual information through a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries and dependencies\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd # To create dataframes\n",
    "import numpy as np # To manipulate arrays\n",
    "from numpy import expand_dims\n",
    "\n",
    "import os # To manipulate path\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm # To produce progress bars\n",
    "import pickle\n",
    "import random\n",
    "from random import sample \n",
    "\n",
    "from random import sample \n",
    "from pathlib import Path # To fix path compatibility issues Windows/linux\n",
    "\n",
    "#a supprimer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "import fnmatch\n",
    "import math \n",
    "\n",
    "#from jupyter_client.manager import KernelManager\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import random\n",
    "from random import sample \n",
    "random.seed(1000)\n",
    "\n",
    "from datetime import date # To include the date in the filenames\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import csv\n",
    "import ntpath\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm # To produce progress bars adpted to Jupyter notebook\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path # To fix path compatibility issues Windows/linux\n",
    "\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "from warnings import simplefilter # import warnings filter\n",
    "simplefilter(action='ignore', category=FutureWarning) # ignore all future warnings\n",
    "\n",
    "# Define variables\n",
    "\n",
    "projectDirectory=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/\") # Project path\n",
    "inputDirectory=Path(\"0_data/dtd/images/\") # Location of folders with pictures (in subfolders \"nodefect\" & \"defect\")\n",
    "inputPath=os.path.join(projectDirectory,inputDirectory) # Create the full path to the input pictures\n",
    "\n",
    "outputDirectory=Path(\"2_pipeline/store/\") # Location of the serialized pictures\n",
    "outputPath=os.path.join(projectDirectory,outputDirectory) # Create the full path to the output pictures\n",
    "dtd=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/images/\")\n",
    "\n",
    "IMG_SIZE = 227 # Set the value to resize the pictures, has to fit with the CNN input (227x227 is the default value)\n",
    "\n",
    "DS = pd.DataFrame({\"dataset\" : []}) # Create a dataframe to run the script in batch mode\n",
    "#DS = DS.append({\"dataset\" : \"1\"},ignore_index=True)\n",
    "DS = DS.append({\"dataset\" : \"2\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"3\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"4\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"5\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"6\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"7\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"8\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"9\"},ignore_index=True)\n",
    "#DS = DS.append({\"dataset\" : \"10\"},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset processing\n",
    "\n",
    "- Input : unzipped DTD database available at https://www.robots.ox.ac.uk/~vgg/data/dtd/\n",
    "- Output : Serialized dataset ready-to-learn (files = x3(train, validation, test) x2(image, label) x10(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_size(start_path = '.'): # Check the folders size\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def create_training_data(): # Create the dataset for training (picture, label)\n",
    "    for category in categories:  # For each folder, create a number for each category\n",
    "        #path = os.path.join(DATA, category)  \n",
    "        class_num = categories.index(category) \n",
    "        #for img in tqdm(os.path.join(category,os.path.split(lines)[1])):  # for each image in each folder, create array with label\n",
    "        for line in lines:\n",
    "            if category == os.path.split(line)[0]:\n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(Path(dtd,category,os.path.split(line)[1])))  # open the image in grayscale\n",
    "                    #new_array = img_array[0:IMG_SIZE, 0:IMG_SIZE]\n",
    "                    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize the image\n",
    "                    new_array90 = np.rot90(new_array)\n",
    "                    new_array180 = np.rot90(new_array90)\n",
    "                    new_array270 = np.rot90(new_array180)\n",
    "                    training_data.append([new_array, class_num])  # add it to the training dataset\n",
    "                    training_data.append([new_array90, class_num])  # add it to the training dataset\n",
    "                    training_data.append([new_array180, class_num])  # add it to the training dataset\n",
    "                    training_data.append([new_array270, class_num])  # add it to the training dataset\n",
    "                except Exception as e:  # To keep the output clean\n",
    "                    pass\n",
    "                            \n",
    "i = 0\n",
    "while i < DS.count()[0]: # For the number of entries in the DS dataframe:\n",
    "    for serie in [\"train\", \"test\", \"val\"]:\n",
    "        lines = []\n",
    "        buffer=[]\n",
    "        with open(os.path.join(Path('C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/labels',serie+DS.iloc[0][\"dataset\"]+'.txt'))) as f:\n",
    "            lines = [line.rstrip() for line in f]\n",
    "            for image in lines:\n",
    "                #print(os.path.join(Path(dtd,image)))\n",
    "                #tmp = plt.imread(os.path.join(Path(dtd,image)))\n",
    "                #plt.imshow(tmp)\n",
    "                #plt.show()\n",
    "                #print(os.path.split(image)[0])\n",
    "                buffer.append(os.path.split(image)[0])\n",
    "            categories = list( dict.fromkeys(buffer) )\n",
    "            print(\"categories are :\", categories)\n",
    "            #DATA=os.path.join(inputPath, DS.iloc[i][\"dataset\"]) # Create a path to the cata\n",
    "            \n",
    "            training_data = [] # Create an empty matrix    \n",
    "            create_training_data() # Create the dataset for training (picture, label)\n",
    "            random.shuffle(training_data) # randomize the data\n",
    "            datafeatures = [] # Create an empty vector for features training set\n",
    "            trainDataLabel = [] # Create an empty vector for label training set\n",
    "            for features,label in training_data:\n",
    "                datafeatures.append(features)\n",
    "                trainDataLabel.append(label)\n",
    "            datafeatures = np.array(datafeatures).reshape(-1, IMG_SIZE, IMG_SIZE, 3) # reshape the data to fit with TensorFlow. (Grayscale=1, RGB=3)        \n",
    "            \n",
    "            datafeaturesName=os.path.join(DS.iloc[i][\"dataset\"][0] + \"_\"+serie+\"datafeatures.pickle\") # Generate a comprehensive name for serialized data\n",
    "            datafeaturesPath=os.path.join(outputPath,datafeaturesName) # Create the full path to serialize the data\n",
    "            pickle_out = open(datafeaturesPath,\"wb\") # Open path to serialize the data\n",
    "            pickle.dump(datafeatures, pickle_out) # Serialize the data\n",
    "            pickle_out.close()\n",
    "            \n",
    "            datalabelName=os.path.join(DS.iloc[i][\"dataset\"][0] + \"_\"+serie+\"datalabel.pickle\") # Generate a comprehensive name for serialized labels\n",
    "            datalabelPath=os.path.join(outputPath,datalabelName)# Create the full path to serialize the labels\n",
    "            pickle_out = open(datalabelPath,\"wb\") # Open path to serialize the labels\n",
    "            pickle.dump(trainDataLabel, pickle_out) # Serialize the labels\n",
    "            pickle_out.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters to specify\n",
    "projectDirectory=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/\") # Project path\n",
    "inputDirectory=Path(\"2_pipeline/store/\") # Location of folders with pictures (in subfolders \"nodefect\" & \"defect\")\n",
    "inputPath=Path(projectDirectory/inputDirectory) # Create the full path to the input pictures\n",
    "#MODEL=Path(projectDirectory / \"mod/\") # Where to save the dataset\n",
    "#LOG=Path(projectDirectory / \"log/\")\n",
    "IMG_SIZE = 227 # has to fit with the value in \"6_serialize_dataset\"\n",
    "outputDirectory=Path(\"2_pipeline/store/\") # Location to save the models and logs\n",
    "outputPath=Path(projectDirectory/outputDirectory) # Create the full path to the output pictures\n",
    "\n",
    "# Define the design of experiments\n",
    "DOE = pd.DataFrame({\"dataset\" : []}) # Create a dataframe to run the script in batch mode\n",
    "#DOE = DOE.append({\"dataset\" : \"1\"},ignore_index=True)\n",
    "DOE = DOE.append({\"dataset\" : \"2\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"3\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"4\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"5\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"6\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"7\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"8\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"9\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"10\"},ignore_index=True)\n",
    "\n",
    "## Training the model\n",
    "i = 0\n",
    "while i < DOE.count()[0]:\n",
    "    # File and folder locations\n",
    "    trainDataLabel = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_traindatalabel.pickle\")),\"rb\"))\n",
    "    valDataLabel = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_valdatalabel.pickle\")),\"rb\"))\n",
    "    \n",
    "    trainDataFeatures = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_traindatafeatures.pickle\")),\"rb\"))\n",
    "    valDataFeatures = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_valdatafeatures.pickle\")),\"rb\"))\n",
    "\n",
    "    ## Build the model: \n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st layer\n",
    "    model.add(Conv2D(filters=96, \n",
    "                     input_shape=(IMG_SIZE,IMG_SIZE,3), \n",
    "                     kernel_size=(11,11), \n",
    "                     strides=(4,4), \n",
    "                     padding=\"valid\", \n",
    "                     data_format=\"channels_last\", \n",
    "                     name='conv1'\n",
    "                    )\n",
    "             )\n",
    "    model.add(Activation('relu', name='activ1'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\", data_format=\"channels_last\", name='pool1'))\n",
    "    model.add(BatchNormalization(name='normal1'))\n",
    "\n",
    "    # 2nd layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv2'))\n",
    "    model.add(Activation('relu', name='activ2'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\", data_format=\"channels_last\", name='pool2'))\n",
    "    model.add(BatchNormalization(name='normal2'))\n",
    "\n",
    "    # 3rd layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv3'))\n",
    "    model.add(Activation('relu', name='activ3'))\n",
    "    model.add(BatchNormalization(name='normal3'))\n",
    "\n",
    "    # 4th layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv4'))\n",
    "    model.add(Activation('relu', name='activ4'))\n",
    "    model.add(BatchNormalization(name='normal4'))\n",
    "\n",
    "    # 5th layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv5'))\n",
    "    model.add(Activation('relu', name='activ5'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\", data_format=\"channels_last\", name='pool3'))\n",
    "    model.add(BatchNormalization(name='normal5'))\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten(name='flat1'))\n",
    "\n",
    "    # 6th layer\n",
    "    model.add(Dense(4096, \n",
    "                    input_shape=(IMG_SIZE*IMG_SIZE*3,), \n",
    "                    name='dense1'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    model.add(Activation('relu', name='activ6'))\n",
    "    model.add(Dropout(0.5, name='drop1')) #to prevent overfitting\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 6th layer\n",
    "    model.add(Dense(4096, name='dense2'))\n",
    "    model.add(Activation('relu', name='activ7'))\n",
    "    model.add(Dropout(0.5, name='drop2')) #to prevent overfitting\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 7th layer\n",
    "    model.add(Dense(4096, name='dense3'))\n",
    "    model.add(Activation('relu', name='activ8'))\n",
    "    model.add(Dropout(0.5, name='drop3'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(47, name='dense4'))\n",
    "    model.add(Activation('softmax', name='activ9'))\n",
    "\n",
    "    #for i, layer in enumerate(model.layers): #To rename all layers\n",
    "    #    layer.name = 'layer_' + str(i)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(  \n",
    "        loss='sparse_categorical_crossentropy', # can be 'binary_crossentropy','sparse_categorical_crossentropy','categorical_crossentropy'\n",
    "        optimizer='sgd', # can be'sgd' or 'adam'\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.run_eagerly = False # To allow the use of callbacks tensorboard\n",
    "\n",
    "    date=str(datetime.now().strftime(\"%Y%m%d_%H%M%S\")) # Generate a unique name based on date and time to save the generated files \n",
    "    #savePath=Path(outputPath / date)\n",
    "    #savePath.mkdir\n",
    "    #modeldir=Path(MODEL / date)\n",
    "\n",
    "    logpath=Path(outputPath / date)\n",
    "    logpath.mkdir()\n",
    "\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=logpath)\n",
    "    \n",
    "    hist=model.fit(trainDataFeatures, trainDataLabel, \n",
    "                   callbacks=[tensorboard],\n",
    "              batch_size=64, # Number of images considered for each epoch\n",
    "              epochs=500, # Number of runs to train the model\n",
    "              verbose=1, #Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "              #validation_split=0.1,\n",
    "              validation_data=(valDataFeatures,valDataLabel), # Ratio trained/tested data\n",
    "              shuffle=False, #whether to shuffle the training data before each epoch\n",
    "             )\n",
    "    #modeldir.mkdir()\n",
    "    modelname=os.path.join(date + \"_exp2_dtdRotated.model\")\n",
    "    model.save(Path(outputPath / modelname))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to specify\n",
    "\n",
    "projectDirectory=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/\") # Project path\n",
    "inputDirectory=Path(\"2_pipeline/store/\") # Location of the model\n",
    "modelName=Path(\"20200405_162613_exp2_BS64.model\") # Name of the model\n",
    "\n",
    "outputDirectory=Path(\"2_pipeline/store/\") # Location to save the figures\n",
    "storeDirectory=Path(\"2_pipeline/store/\") # Location to save the intermediate dataframes\n",
    "figPath = \"C:/Users/AQ62270/201910_ccn/NIPS/2_pipeline/store/\" # Location of saved figures for the paper\n",
    "\n",
    "dataDirectory=Path('C:/Users/AQ62270/201910_ccn/0_data/processed/500-300-0Light_Clean/') # Specify the dataset to evaluate the model\n",
    "\n",
    "inputPath=Path(projectDirectory/inputDirectory) # Create the full path to the input pictures\n",
    "outputPath=Path(projectDirectory/outputDirectory) # Create the full path to the output pictures\n",
    "storePath=Path(projectDirectory/storeDirectory) # Create the full path to the output pictures\n",
    "\n",
    "intermediateResults=True\n",
    "\n",
    "# Define design\n",
    "## Font size\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "## Figure dimensions\n",
    "pageWidth=174/25.4\n",
    "columnWidth=84/25.4\n",
    "baseLineNumbering=(0.5,-0.28) # Position of the captions under the sub-figures (letters or numbers)\n",
    "\n",
    "## Colors\n",
    "colorValSplit10=\"#d8b365\"\n",
    "colorValSplit30=\"#5ab4ac\"\n",
    "markerSplit10=\"o\"\n",
    "markerSplit30=\"s\"\n",
    "colorDefect=\"#f1a340\"\n",
    "colorNodefect=\"#998ec3\"\n",
    "\n",
    "# Open the model\n",
    "modelPath=os.path.join(inputPath,modelName)\n",
    "model = keras.models.load_model(modelPath)\n",
    "model.summary()\n",
    "# TO DO - plot the learning curve \n",
    "#plt.plot(hist.history['val_accuracy'])\n",
    "\n",
    "# Get the convolution layers adresses in the model\n",
    "convLayers=pd.DataFrame({\"layerIndex\":[], \"convolutionIndex\":[], \"layerName\":[]}, dtype=int)\n",
    "i=0\n",
    "j=1\n",
    "for layer in model.layers:\n",
    "    if \"conv\" in model.layers[i].name:\n",
    "        convLayers=convLayers.append({\"layerIndex\":i, \"convolutionIndex\":j, \"layerName\":model.layers[i].name}, ignore_index=True)\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "layerNumber=[0,4,8,11,14] # Manually select the adress of convolution layers\n",
    "# TO DO - Do it automatically\n",
    "# layerNumber=convLayer.loc[].count\n",
    "\n",
    "# List the categories (If not done previously)\n",
    "lines = []\n",
    "buffer=[]\n",
    "with open(os.path.join(Path('C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/labels/test'+DS.iloc[0][\"dataset\"]+'.txt'))) as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    for image in lines:\n",
    "        buffer.append(os.path.split(image)[0])\n",
    "        categories = list( dict.fromkeys(buffer) )\n",
    "\n",
    "if intermediateResults==True:\n",
    "    print(convLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save maximal value of the convolutional images for each convolution layer\n",
    "\n",
    "dfFeatureMap = pd.DataFrame({\"Layer\":[], \"filtre\" : [], \"image\":[], \"category\":[], \"maxima\":[]}, dtype=int)      \n",
    "\n",
    "# Create \"n\" submodels for \"n\" existing convolution layers\n",
    "for layer in convLayers[\"convolutionIndex\"]:\n",
    "    print(\"Layer number \", layer)\n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    modelLayer = Model(inputs=model.inputs, outputs=model.layers[layerNumber[layer-1]].output) # Create the submodel\n",
    "    \n",
    "    for category in tqdm(categories):  # For each folder, create a number for each category\n",
    "        class_num = categories.index(category) \n",
    "        for line in lines:\n",
    "            trueCategory = os.path.split(line)[0]\n",
    "            if category == trueCategory:\n",
    "                try:\n",
    "                    imageName=os.path.split(line)[1]\n",
    "                    img_array = cv2.imread(os.path.join(Path(dtd,category,os.path.split(line)[1])))  # open the image in grayscale\n",
    "                    img_array= cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    img_array = expand_dims(img_array, axis=0)                    \n",
    "                    for filterNumber in range(numberOfFilter):\n",
    "                        feature_maps = modelLayer.predict(img_array)\n",
    "                        dfFeatureMap=dfFeatureMap.append({\"Layer\": layer,\"filtre\" : filterNumber, \"image\":imageName, \"category\":category, \"maxima\":np.max(feature_maps[0, :, :, filterNumber])}, ignore_index=True)\n",
    "                except Exception as e:  # To keep the output clean\n",
    "                    pass\n",
    "    dfFeatureMap.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")),index=True)\n",
    "print(dfFeatureMap)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "if len(dfFeatureMap)==(47*(256*2+384*2+96)*40):\n",
    "    print(dfFeatureMap)\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Warning : Check the dataframe length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimized value mutual information threshold for each filter\n",
    "\n",
    "binsValue=30\n",
    "\n",
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "iconfusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[],\"category\":[], \"iThreshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"iInfoMutuelle\":[]}, dtype=int)\n",
    "\n",
    "for layer in convLayers[\"convolutionIndex\"][1:2]:\n",
    "    print(\"Layer number : \", layer)    \n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    \n",
    "    for filterNumber in range (numberOfFilter)[232:]:\n",
    "        print(\"filter number : \",filterNumber)\n",
    "        for category in categories:\n",
    "            print(\"category : \",category)\n",
    "            #dfinodefect=dfFeatureMap[(dfFeatureMap['category']==\"nodefect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            #dfidefect=dfFeatureMap[(dfFeatureMap['category']==\"defect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            minND=np.amin(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            maxND=np.amax(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            thresholds=np.arange(minND, maxND, (maxND-minND)/20)\n",
    "            for iThreshold in (thresholds):\n",
    "                iTP=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iTN=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iFP=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iFN=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iS=iTP+iTN+iFP+iFN\n",
    "                label_true=[*np.ones(iTP),*np.zeros(iTN),*np.zeros(iFP),*np.ones(iFN)]\n",
    "                label_pred=[*np.ones(iTP),*np.zeros(iTN),*np.ones(iFP),*np.zeros(iFN)]\n",
    "                \n",
    "                imutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "                iPY1 = (iTP+iFP)/iS\n",
    "                iPY0 = (iFN+iTN)/iS\n",
    "                iPC1 = (iTP+iFN)/iS\n",
    "                iPC0 = (iFP+iTN)/iS\n",
    "                iPY1C1, iPY0C1, iPY1C0, iPY0C0 = iTP/iS, iFN/iS, iFP/iS, iTN/iS\n",
    "                \n",
    "                if (iFP!=0 and iFN!=0 and iTP!=0 and iTN!=0):\n",
    "                    iHY = - iPY1*math.log2(iPY1) - iPY0*math.log2(iPY0)\n",
    "                    iHC = - iPC1*math.log2(iPC1) - iPC0*math.log2(iPC0)\n",
    "                    iHYC = - iPY1C1*math.log2(iPY1C1) - iPY0C1*math.log2(iPY0C1) - iPY1C0*math.log2(iPY1C0) - iPY0C0*math.log2(iPY0C0)\n",
    "                    iIYC = iHY + iHC - iHYC\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":imutualScore, \"iInfoMutuelle\":iIYC}, ignore_index=True)        \n",
    "                else :\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":np.NaN, \"iInfoMutuelle\":np.NaN}, ignore_index=True)        \n",
    "                   \n",
    "        #if intermediateResults==True:\n",
    "        #    for category in categories[43:44]:\n",
    "        #        fig, ax1 = plt.subplots()\n",
    "        #        fig.set_figheight(4)\n",
    "        #        fig.set_figwidth(4)\n",
    "        #        ax1.hist(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"], bins=binsValue, alpha=0.8, density=True, label=category)\n",
    "        #        labelNoCategory=(\"no \"+category)\n",
    "        #        ax1.hist(dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"], bins=binsValue, alpha=0.8, density=True, label=labelNoCategory)\n",
    "        #        ax1.legend(loc=2)\n",
    "        #        ax1.set_ylabel(\"Frequency\")\n",
    "        #        ax1.set_xlabel(\"Max. value\")\n",
    "        #        ax1.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "        #        ax2 = ax1.twinx()\n",
    "        #        ax2.plot(iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iThreshold\"], iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iInfoMutuelle\"], '--', label=\"mutualInfo\", color=\"black\")\n",
    "        #        ax2.set_ylabel(\"Mutual information\")\n",
    "        #        ax2.legend(loc=1)\n",
    "        #        plt.title(\"Filter %i\"% filterNumber + \" of layer %i\" % layer)\n",
    "        #        plt.plot()\n",
    "        #        plt.show()\n",
    "        #        plt.close()\n",
    "\n",
    "    iconfusionMatrix.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix_L2F232_end.csv\")),index=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the optimized value mutual information threshold for each filter\n",
    "\n",
    "binsValue=30\n",
    "categories=[\"spiralled\"]\n",
    "filters=[124,125]\n",
    "layers=[2]\n",
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "iconfusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[],\"category\":[], \"iThreshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"iInfoMutuelle\":[]}, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(pageWidth,3))\n",
    "for layer in layers:\n",
    "    print(\"Layer number : \", layer)    \n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    i=0\n",
    "    for filterNumber in filters:\n",
    "        print(\"filter number : \",filterNumber)\n",
    "#        for category in categories:\n",
    "#            print(\"category : \",category)\n",
    "#            #dfinodefect=dfFeatureMap[(dfFeatureMap['category']==\"nodefect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "#            #dfidefect=dfFeatureMap[(dfFeatureMap['category']==\"defect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "#            minND=np.amin(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "#            maxND=np.amax(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "#            thresholds=np.arange(minND, maxND, (maxND-minND)/20)\n",
    "#            for iThreshold in (thresholds):\n",
    "#                iTP=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "#                iTN=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "#                iFP=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "#                iFN=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "#                iS=iTP+iTN+iFP+iFN\n",
    "#                label_true=[*np.ones(iTP),*np.zeros(iTN),*np.zeros(iFP),*np.ones(iFN)]\n",
    "#                label_pred=[*np.ones(iTP),*np.zeros(iTN),*np.ones(iFP),*np.zeros(iFN)]\n",
    "#                \n",
    "#                imutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "#                iPY1 = (iTP+iFP)/iS\n",
    "#                iPY0 = (iFN+iTN)/iS\n",
    "#                iPC1 = (iTP+iFN)/iS\n",
    "#                iPC0 = (iFP+iTN)/iS\n",
    "#                iPY1C1, iPY0C1, iPY1C0, iPY0C0 = iTP/iS, iFN/iS, iFP/iS, iTN/iS\n",
    "#                \n",
    "#                if (iFP!=0 and iFN!=0 and iTP!=0 and iTN!=0):\n",
    "#                    iHY = - iPY1*math.log2(iPY1) - iPY0*math.log2(iPY0)\n",
    "#                    iHC = - iPC1*math.log2(iPC1) - iPC0*math.log2(iPC0)\n",
    "#                    iHYC = - iPY1C1*math.log2(iPY1C1) - iPY0C1*math.log2(iPY0C1) - iPY1C0*math.log2(iPY1C0) - iPY0C0*math.log2(iPY0C0)\n",
    "#                    iIYC = iHY + iHC - iHYC\n",
    "#                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":imutualScore, \"iInfoMutuelle\":iIYC}, ignore_index=True)        \n",
    "#                else :\n",
    "#                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":np.NaN, \"iInfoMutuelle\":np.NaN}, ignore_index=True)        \n",
    "#                   \n",
    "        for category in categories:\n",
    "            fig, ax1 = plt.subplots()\n",
    "            fig.set_figheight(4)\n",
    "            fig.set_figwidth(4)\n",
    "            axes[i].hist(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"],color=colorDefect, bins=binsValue, alpha=0.8, density=True, label=category)\n",
    "            labelNoCategory=(\"no \"+category)\n",
    "            axes[i].hist(dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"],color=colorNodefect, bins=binsValue, alpha=0.8, density=True, label=labelNoCategory)\n",
    "            axes[i].legend(loc=2)\n",
    "            axes[i].set_ylabel(\"Frequency\")\n",
    "            axes[i].set_xlabel(\"Max. value\")\n",
    "            axes[i].yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "            ax2=axes[i].twinx()\n",
    "            ax2.set_xticks(np.arrange(0,0.4,step=0.01))\n",
    "            ax2.plot(iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iThreshold\"], iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iInfoMutuelle\"], '--', label=\"mutualInfo\", color=\"black\")\n",
    "            ax2.set_ylabel(\"Mutual information\")\n",
    "            #ax2.legend(loc=1)          \n",
    "            axes[i].set_title(\"Filter %i\"% filterNumber + \" of layer %i\" % layer)\n",
    "        i+=1\n",
    "    plt.tight_layout()\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    #iconfusionMatrix.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix_L2F232_end.csv\")),index=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iconfusionMatrix.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix_L2F134_231.csv\")),index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimized value mutual information threshold for each filter\n",
    "\n",
    "binsValue=30\n",
    "\n",
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "iconfusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[],\"category\":[], \"iThreshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"iInfoMutuelle\":[]}, dtype=int)\n",
    "\n",
    "for layer in convLayers[\"convolutionIndex\"][3:4]:\n",
    "    print(\"Layer number : \", layer)    \n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    \n",
    "    for filterNumber in range (numberOfFilter)[333:]:\n",
    "        print(\"filter number : \",filterNumber)\n",
    "        for category in categories:\n",
    "            print(\"category : \",category)\n",
    "            #dfinodefect=dfFeatureMap[(dfFeatureMap['category']==\"nodefect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            #dfidefect=dfFeatureMap[(dfFeatureMap['category']==\"defect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            minND=np.amin(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            maxND=np.amax(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            thresholds=np.arange(minND, maxND, (maxND-minND)/20)\n",
    "            for iThreshold in (thresholds):\n",
    "                iTP=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iTN=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iFP=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iFN=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iS=iTP+iTN+iFP+iFN\n",
    "                label_true=[*np.ones(iTP),*np.zeros(iTN),*np.zeros(iFP),*np.ones(iFN)]\n",
    "                label_pred=[*np.ones(iTP),*np.zeros(iTN),*np.ones(iFP),*np.zeros(iFN)]\n",
    "                \n",
    "                imutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "                iPY1 = (iTP+iFP)/iS\n",
    "                iPY0 = (iFN+iTN)/iS\n",
    "                iPC1 = (iTP+iFN)/iS\n",
    "                iPC0 = (iFP+iTN)/iS\n",
    "                iPY1C1, iPY0C1, iPY1C0, iPY0C0 = iTP/iS, iFN/iS, iFP/iS, iTN/iS\n",
    "                \n",
    "                if (iFP!=0 and iFN!=0 and iTP!=0 and iTN!=0):\n",
    "                    iHY = - iPY1*math.log2(iPY1) - iPY0*math.log2(iPY0)\n",
    "                    iHC = - iPC1*math.log2(iPC1) - iPC0*math.log2(iPC0)\n",
    "                    iHYC = - iPY1C1*math.log2(iPY1C1) - iPY0C1*math.log2(iPY0C1) - iPY1C0*math.log2(iPY1C0) - iPY0C0*math.log2(iPY0C0)\n",
    "                    iIYC = iHY + iHC - iHYC\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":imutualScore, \"iInfoMutuelle\":iIYC}, ignore_index=True)        \n",
    "                else :\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":np.NaN, \"iInfoMutuelle\":np.NaN}, ignore_index=True)        \n",
    "                   \n",
    "        #if intermediateResults==True:\n",
    "        #    for category in categories[43:44]:\n",
    "        #        fig, ax1 = plt.subplots()\n",
    "        #        fig.set_figheight(4)\n",
    "        #        fig.set_figwidth(4)\n",
    "        #        ax1.hist(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"], bins=binsValue, alpha=0.8, density=True, label=category)\n",
    "        #        labelNoCategory=(\"no \"+category)\n",
    "        #        ax1.hist(dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"], bins=binsValue, alpha=0.8, density=True, label=labelNoCategory)\n",
    "        #        ax1.legend(loc=2)\n",
    "        #        ax1.set_ylabel(\"Frequency\")\n",
    "        #        ax1.set_xlabel(\"Max. value\")\n",
    "        #        ax1.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "        #        ax2 = ax1.twinx()\n",
    "        #        ax2.plot(iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iThreshold\"], iconfusionMatrix[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)][\"iInfoMutuelle\"], '--', label=\"mutualInfo\", color=\"black\")\n",
    "        #        ax2.set_ylabel(\"Mutual information\")\n",
    "        #        ax2.legend(loc=1)\n",
    "        #        plt.title(\"Filter %i\"% filterNumber + \" of layer %i\" % layer)\n",
    "        #        plt.plot()\n",
    "        #        plt.show()\n",
    "        #        plt.close()\n",
    "\n",
    "    iconfusionMatrix.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix_L4F108_end.csv\")),index=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built the confusion matrix with optimized value of mutual information for each filter\n",
    "\n",
    "iconfusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix.txt\")))\n",
    "confusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[], \"category\":[], \"threshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"mutualInfo\":[]}, dtype=int)\n",
    "\n",
    "for layer in convLayers[\"convolutionIndex\"][2:3]:\n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    print(layer)\n",
    "    for filterNumber in range (133,384):\n",
    "        print(filterNumber)\n",
    "        for category in categories:\n",
    "            threshold=iconfusionMatrix.loc[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)].sort_values(by=\"iInfoMutuelle\", ascending=False).iloc[0][\"iThreshold\"]\n",
    "            \n",
    "            TP=dfFeatureMap[(dfFeatureMap['category']==category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=threshold)].count()[4]\n",
    "            TN=dfFeatureMap[(dfFeatureMap['category']!=category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<threshold)].count()[4]\n",
    "            FP=dfFeatureMap[(dfFeatureMap['category']!=category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=threshold)].count()[4]\n",
    "            FN=dfFeatureMap[(dfFeatureMap['category']==category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<threshold)].count()[4]\n",
    "            S=TP+TN+FP+FN\n",
    "            \n",
    "            # Calcul of the mutual information\n",
    "            PY1 = (TP+FP)/S\n",
    "            PY0 = (FN+TN)/S\n",
    "            PC1 = (TP+FN)/S\n",
    "            PC0 = (FP+TN)/S \n",
    "            PY1C1, PY0C1, PY1C0, PY0C0 = TP/S, FN/S, FP/S, TN/S\n",
    "            \n",
    "            HY = - PY1*math.log2(PY1) - PY0*math.log2(PY0)\n",
    "            HC = - PC1*math.log2(PC1) - PC0*math.log2(PC0)\n",
    "            HYC = - PY1C1*math.log2(PY1C1) - PY0C1*math.log2(PY0C1) - PY1C0*math.log2(PY1C0) - PY0C0*math.log2(PY0C0)     \n",
    "            IYC = HY + HC - HYC\n",
    "        \n",
    "            label_true=[*np.ones(TP),*np.zeros(TN),*np.zeros(FP),*np.ones(FN)]\n",
    "            label_pred=[*np.ones(TP),*np.zeros(TN),*np.ones(FP),*np.zeros(FN)]\n",
    "            mutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "    \n",
    "            confusionMatrix=confusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"threshold\":threshold, \"truePositive\":TP,\"trueNegative\":TN, \"falsePositive\":FP, \"falsenegative\":FN, \"mutualScore\":mutualScore, \"mutualInfo\":IYC}, ignore_index=True)\n",
    "\n",
    "confusionMatrix.to_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrixtest.csv\")),index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entropy histogram\n",
    "confusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")))\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "\n",
    "fig, axes = plt.subplots(3,2,figsize=(pageWidth,7))\n",
    "\n",
    "i=0.1\n",
    "j=0\n",
    "for category in categories[:3]:\n",
    "    tab=pd.DataFrame({\"layer\":[], \"decile\":[]})\n",
    "\n",
    "    for layer in convLayers[\"convolutionIndex\"]:\n",
    "    \n",
    "        axes[j,0].hist(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)].sort_values(by=\"mutualInfo\", ascending=False)[\"mutualInfo\"], bins=30, alpha=0.7, color=viridis(i), density=True, label={\"Layer %i\" %layer})\n",
    "        tab=tab.append({\"layer\":layer, \"decile\":np.percentile(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)][\"mutualInfo\"],np.arange(0,100,10))[9]},ignore_index=True)\n",
    "        axes[j,1].scatter(tab[\"layer\"][layer-1],tab[\"decile\"][layer-1],color=viridis(i),s=70, label=\"Layer %i\" %layer)\n",
    "        i+=0.3\n",
    "        \n",
    "        axes[j,0].set_ylabel(\"Frequency\")\n",
    "        axes[j,0].set_xlabel(\"Mutual information\")\n",
    "        axes[j,0].legend()\n",
    "        #axes[j,0].text(baseLineNumbering[0],baseLineNumbering[1],\"(a)\", horizontalalignment='center',verticalalignment='center', transform=axes[0].transAxes)\n",
    "\n",
    "        axes[j,1].yaxis.set_label_position(\"right\")\n",
    "        axes[j,1].yaxis.tick_right()\n",
    "        #axes[j,1].text(baseLineNumbering[0],baseLineNumbering[1],\"(b)\", horizontalalignment='center',verticalalignment='center', transform=axes[1].transAxes)\n",
    "        axes[j,1].legend()\n",
    "        axes[j,1].set_xlabel(\"Layer\")\n",
    "        axes[j,1].set_xlim(0,6)\n",
    "        axes[j,1].set_xticks(np.arange(1, 6, step=1))\n",
    "        axes[j,1].set_ylabel(\"Mutual information \\n of last decile\")\n",
    "    j+=1\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(os.path.join(figPath,os.path.splitext(modelName)[0]+\"_mutualInfoLayers.pdf\")))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mutual information (last decile) through the Newkork (sorted) - Pandas version - DRAFT\n",
    "\n",
    "#confusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")))\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "fig, ax = plt.subplots(1,1,figsize=(pageWidth*2,1))\n",
    "i=0.1\n",
    "layers = [1,2,3,4]\n",
    "tab=pd.DataFrame({\"layer\":[],\"category\":[], \"decile\":[], \"category\":[]})\n",
    "\n",
    "for layer in layers:\n",
    "    for category in categories:    \n",
    "        tab=tab.append({\"layer\":layer, \"category\":category, \"decile\":np.percentile(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)][\"mutualInfo\"],np.arange(0,100,10))[9], \"category\":category},ignore_index=True)\n",
    "\n",
    "x=[]\n",
    "for layer in (layers):\n",
    "    x1=tab[tab[\"layer\"]==layer].sort_values(by=\"category\", ascending=True)[\"decile\"]\n",
    "    x=np.append(x,x1,axis=0)\n",
    "x=x.reshape(4,47)\n",
    "g=x[:,np.argsort(x[0])]\n",
    "im = ax.imshow(x)\n",
    "ax.set_yticks(np.arange(0,len(layers)))\n",
    "ax.set_yticklabels(layers)\n",
    "ax.set_xticks(np.arange(len(categories)))\n",
    "ax.set_xticklabels(tab[tab[\"layer\"]==layer].sort_values(by=\"category\", ascending=True)[\"category\"])\n",
    "ax.set_ylabel('Layers')\n",
    "ax.set_title(\"Mutual information score through the network (last decile)\")\n",
    "ax.vlines(np.arange(-0.5,len(categories)-1), -.5, len(layers)-.5, 'w')\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, va=\"center\", ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "cbar=fig.colorbar(im,ax=ax, pad=0.01, aspect=5, ticks=[0,0.01,0.02])\n",
    "cbar.set_ticks([0,0.01,0.02])\n",
    "cbar.set_ticklabels([0,0.01,0.02])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entropy all the histograms by category \n",
    "\n",
    "confusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")))\n",
    "layers = [1,2,3,4,5]\n",
    "for nbOfGraph in range (0,10):\n",
    "    viridis = cm.get_cmap('viridis', 5)\n",
    "    \n",
    "    fig, axes = plt.subplots(2,6,figsize=(pageWidth,3))\n",
    "    \n",
    "    tab=pd.DataFrame({\"layer\":[], \"decile\":[], \"category\":[]})\n",
    "    subcategories=categories[5*nbOfGraph:5+5*nbOfGraph]\n",
    "    \n",
    "    layerLegend=[]\n",
    "    for layer in layers:\n",
    "        layerLegend.append(Line2D([0], [0], color=viridis(layers.index(layer)/len(layers)), lw=4))\n",
    "        for category in subcategories:\n",
    "            ax_category=axes[0,subcategories.index(category)] # Specify the subplot object\n",
    "            sns.distplot(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)].sort_values(by=\"mutualInfo\", ascending=False)[\"mutualInfo\"], \n",
    "                         ax=ax_category, \n",
    "                         color=viridis(layers.index(layer)/len(layers)),\n",
    "                         bins=30, \n",
    "                         hist=False,\n",
    "                        )\n",
    "            sns.despine(ax=ax_category,top=True, right=True, left=True, bottom=False)\n",
    "            ax_category.set_xlabel(\"%s\" %category)\n",
    "            ax_category.xaxis.set_ticks_position('bottom')\n",
    "            ax_category.xaxis.set_label_position('top')\n",
    "            #axes0,[0].text(baseLineNumbering[0],baseLineNumbering[1],\"(a)\", horizontalalignment='center',verticalalignment='center', transform=axes[0].transAxes)\n",
    "        axes[0,0].set_ylabel(\"Frequency\")\n",
    "        axes[0,5].axis('off')\n",
    "        axes[0,5].legend(layerLegend,layers,frameon=False)\n",
    "    categoryLegend=[]\n",
    "    for category in subcategories:\n",
    "        categoryLegend.append(Line2D([0], [0], color=sns.color_palette(\"Set1\", n_colors=len(layers))[subcategories.index(category)], lw=4))\n",
    "        for layer in layers:\n",
    "            ax_layer=axes[1,layers.index(layer)]\n",
    "            sns.distplot(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)].sort_values(by=\"mutualInfo\", ascending=False)[\"mutualInfo\"], \n",
    "                         ax=ax_layer, \n",
    "                         bins=30,\n",
    "                         hist=False,\n",
    "                         color=sns.color_palette(\"Set1\", n_colors=len(layers))[subcategories.index(category)],\n",
    "                        )\n",
    "            \n",
    "            sns.despine(ax=ax_layer,top=True, right=True, left=True, bottom=False)\n",
    "            axes[1,0].set_ylabel(\"Frequency\")\n",
    "            ax_layer.set_xlabel(\"Layer %i\" %layer)\n",
    "            #axes[j,0].text(baseLineNumbering[0],baseLineNumbering[1],\"(a)\", horizontalalignment='center',verticalalignment='center', transform=axes[0].transAxes)\n",
    "        axes[1,5].axis('off')\n",
    "        shortlist=[]\n",
    "        for short in subcategories:\n",
    "            shortlist.append(short[:4])\n",
    "        axes[1,5].legend(categoryLegend,shortlist,frameon=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(Path(os.path.join(figPath,os.path.splitext(modelName)[0]+\"_mutualInfoLayers.pdf\")))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entropy the selected histograms by category \n",
    "\n",
    "confusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")))\n",
    "layers = [1,2,3,4,5]\n",
    "viridis = cm.get_cmap('viridis', 5)\n",
    "\n",
    "fig, axes = plt.subplots(2,6,figsize=(pageWidth,3))\n",
    "\n",
    "tab=pd.DataFrame({\"layer\":[], \"decile\":[], \"category\":[]})\n",
    "subcategories=[\"cobwebbed\",\"knitted\",\"marbled\",\"spiralled\",\"studded\"]\n",
    "\n",
    "layerLegend=[]\n",
    "for layer in layers:\n",
    "    layerLegend.append(Line2D([0], [0], color=viridis(layers.index(layer)/len(layers)), lw=4))\n",
    "    for category in subcategories:\n",
    "        ax_category=axes[0,subcategories.index(category)] # Specify the subplot object\n",
    "        sns.distplot(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)].sort_values(by=\"mutualInfo\", ascending=False)[\"mutualInfo\"], \n",
    "                     ax=ax_category, \n",
    "                     color=viridis(layers.index(layer)/len(layers)),\n",
    "                     bins=30, \n",
    "                     hist=False,\n",
    "                    )\n",
    "        sns.despine(ax=ax_category,top=True, right=True, left=True, bottom=False)\n",
    "        ax_category.set_xlabel(\"%s\" %category)\n",
    "        ax_category.xaxis.set_ticks_position('bottom')\n",
    "        ax_category.xaxis.set_label_position('top')\n",
    "        #axes0,[0].text(baseLineNumbering[0],baseLineNumbering[1],\"(a)\", horizontalalignment='center',verticalalignment='center', transform=axes[0].transAxes)\n",
    "    axes[0,0].set_ylabel(\"Frequency\")\n",
    "    axes[0,5].axis('off')\n",
    "    axes[0,5].legend(layerLegend,layers,frameon=False)\n",
    "categoryLegend=[]\n",
    "for category in subcategories:\n",
    "    categoryLegend.append(Line2D([0], [0], color=sns.color_palette(\"Set1\", n_colors=len(layers))[subcategories.index(category)], lw=4))\n",
    "    for layer in layers:\n",
    "        ax_layer=axes[1,layers.index(layer)]\n",
    "        sns.distplot(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)].sort_values(by=\"mutualInfo\", ascending=False)[\"mutualInfo\"], \n",
    "                     ax=ax_layer, \n",
    "                     bins=30,\n",
    "                     hist=False,\n",
    "                     color=sns.color_palette(\"Set1\", n_colors=len(layers))[subcategories.index(category)],\n",
    "                    )\n",
    "        \n",
    "        sns.despine(ax=ax_layer,top=True, right=True, left=True, bottom=False)\n",
    "        axes[1,0].set_ylabel(\"Frequency\")\n",
    "        ax_layer.set_xlabel(\"Layer %i\" %layer)\n",
    "        #axes[j,0].text(baseLineNumbering[0],baseLineNumbering[1],\"(a)\", horizontalalignment='center',verticalalignment='center', transform=axes[0].transAxes)\n",
    "    axes[1,5].axis('off')\n",
    "    shortlist=[]\n",
    "    for short in subcategories:\n",
    "        shortlist.append(short[:4])\n",
    "    axes[1,5].legend(categoryLegend,shortlist,frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(Path(os.path.join(figPath,os.path.splitext(modelName)[0]+\"_mutualInfoLayersSelected.pdf\")))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mutual information (last decile) through the Newkork (sorted) - Pandas version - DRAFT\n",
    "\n",
    "#confusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")))\n",
    "\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "fig, ax = plt.subplots(1,1,figsize=(pageWidth*2,1))\n",
    "i=0.1\n",
    "layers = [1,2,3,4,5]\n",
    "tab=pd.DataFrame({\"layer\":[],\"category\":[], \"decile\":[], \"category\":[]})\n",
    "\n",
    "for layer in layers:\n",
    "    for category in categories:    \n",
    "        tab=tab.append({\"layer\":layer, \"category\":category, \"decile\":np.percentile(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)][\"mutualInfo\"],np.arange(0,100,10))[9], \"category\":category},ignore_index=True)\n",
    "\n",
    "x=[]\n",
    "for layer in (layers):\n",
    "    x1=tab[tab[\"layer\"]==layer].sort_values(by=\"category\", ascending=True)[\"decile\"]\n",
    "    x=np.append(x,x1,axis=0)\n",
    "x=x.reshape(5,47)\n",
    "g=x[:,np.argsort(x[0])]\n",
    "im = ax.imshow(x)\n",
    "ax.set_yticks(np.arange(0,len(layers)))\n",
    "ax.set_yticklabels(layers)\n",
    "ax.set_xticks(np.arange(len(categories)))\n",
    "ax.set_xticklabels(tab[tab[\"layer\"]==layer].sort_values(by=\"category\", ascending=True)[\"category\"])\n",
    "ax.set_ylabel('Layers')\n",
    "ax.set_title(\"Mutual information score through the network (last decile)\")\n",
    "ax.vlines(np.arange(-0.5,len(categories)-1), -.5, len(layers)-.5, 'w')\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, va=\"center\", ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "cbar=fig.colorbar(im,ax=ax, pad=0.01, aspect=5, ticks=[0,0.01,0.02])\n",
    "cbar.set_ticks([0,0.01,0.02])\n",
    "cbar.set_ticklabels([0,0.01,0.02])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(Path(os.path.join(figPath,os.path.splitext(modelName)[0]+\"_LastDecileMap.pdf\")))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "from PIL import Image \n",
    "import sys\n",
    "\n",
    "layer=0\n",
    "numberOfClasses=5\n",
    "imagesPerCategory=3\n",
    "IMG_SIZE=350\n",
    "numberOfColumns=1+imagesPerCategory\n",
    "\n",
    "fig, axes = plt.subplots(numberOfColumns,2,figsize=(pageWidth,5))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "tab=pd.DataFrame({\"category\":[],\n",
    "                  \"decile\":[]\n",
    "                 })\n",
    "# Highest values\n",
    "for category in sortedCategory[::-1]:\n",
    "    decile=np.percentile(confusionMatrix[(confusionMatrix[\"Layer\"]==layer+1)&(confusionMatrix[\"category\"]==category)][\"mutualInfo\"],np.arange(0,100,10))[9]\n",
    "    tab=tab.append({\"category\":category, \n",
    "                    \"decile\":decile,\n",
    "                   },\n",
    "                   ignore_index=True)\n",
    "    tab=tab.sort_values(by=\"decile\", ascending=False)\n",
    "\n",
    "axes[0,0].bar(tab[:numberOfClasses][\"category\"],\n",
    "                tab[:numberOfClasses][\"decile\"])\n",
    "#axes[0,0].axis('off')\n",
    "axes[0,0].set_yticks([0,0.01,0.02])\n",
    "axes[0,0].set_ylabel(\"Mutual \\n information\")\n",
    "ax.set_yticklabels(layers)\n",
    "#axes[0,0].get_shared_y_axes().join(axes[0,0], axes[0,1])\n",
    "plt.setp(axes[0,0].get_xticklabels(), \n",
    "         rotation=45, \n",
    "         va=\"center\", \n",
    "         ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "i=1\n",
    "for imageNumber in np.arange(0,imagesPerCategory,step=1):\n",
    "    listImages=[]\n",
    "    #sortedCategory=y[0,np.argsort(x[layer])] #test\n",
    "    #print(sortedCategory)\n",
    "    for category in sortedCategory[::-1][:numberOfClasses]:\n",
    "     #for category in sortedCategory[::][:numberOfClasses]:\n",
    "        #print(category)\n",
    "        imgName=os.listdir(\"/home/quentin/Documents/NIPS/0_data/dtd/images/\"+category+\"/\")[imageNumber]\n",
    "        imgPath = (\"/home/quentin/Documents/NIPS/0_data/dtd/images/\"+category+\"/\"+imgName) \n",
    "        listImages.append(imgPath)\n",
    "    images = [Image.open(x) for x in listImages]\n",
    "    #widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = IMG_SIZE*numberOfClasses\n",
    "    max_height = IMG_SIZE\n",
    "    new_im = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        im = im.resize((IMG_SIZE,IMG_SIZE))\n",
    "        new_im.paste(im, (x_offset,0))\n",
    "        x_offset += im.size[0]\n",
    "    axes[i,0].imshow(new_im)\n",
    "    axes[i,0].axis('off')\n",
    "    i+=1\n",
    "    \n",
    "#Lowest values\n",
    "plt.setp(axes[0,1].get_xticklabels(), \n",
    "         rotation=45, \n",
    "         va=\"center\", \n",
    "         ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "axes[0,1].bar(tab[-numberOfClasses::][\"category\"],\n",
    "                tab[-numberOfClasses::][\"decile\"])\n",
    "#axes[0,1].axis('off')\n",
    "axes[0,1].set_yticks([0,0.01,0.02])\n",
    "i=1\n",
    "for imageNumber in np.arange(0,imagesPerCategory,step=1):\n",
    "    listImages=[]\n",
    "    for category in sortedCategory[numberOfClasses*2-1::-1][numberOfClasses:]: # To rewrite\n",
    "     #for category in sortedCategory[::][:numberOfClasses]:\n",
    "        imgName=os.listdir(\"/home/quentin/Documents/NIPS/0_data/dtd/images/\"+category+\"/\")[imageNumber]\n",
    "        imgPath = (\"/home/quentin/Documents/NIPS/0_data/dtd/images/\"+category+\"/\"+imgName) \n",
    "        listImages.append(imgPath)\n",
    "    images = [Image.open(x) for x in listImages]\n",
    "    #widths, heights = zip(*(i.size for i in images))\n",
    "    total_width = IMG_SIZE*numberOfClasses\n",
    "    max_height = IMG_SIZE\n",
    "    new_im2 = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for im in images:\n",
    "        im = im.resize((IMG_SIZE,IMG_SIZE))\n",
    "        new_im2.paste(im, (x_offset,0))\n",
    "        x_offset += im.size[0]\n",
    "    axes[i,1].imshow(new_im2)\n",
    "    axes[i,1].axis('off')\n",
    "    i+=1\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/quentin/Documents/NIPS/2_pipeline/store/fig_example.pdf\")\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters to specify\n",
    "projectDirectory=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/\") # Project path\n",
    "inputDirectory=Path(\"2_pipeline/store/\") # Location of folders with pictures (in subfolders \"nodefect\" & \"defect\")\n",
    "inputPath=Path(projectDirectory/inputDirectory) # Create the full path to the input pictures\n",
    "#MODEL=Path(projectDirectory / \"mod/\") # Where to save the dataset\n",
    "#LOG=Path(projectDirectory / \"log/\")\n",
    "IMG_SIZE = 227 # has to fit with the value in \"6_serialize_dataset\"\n",
    "outputDirectory=Path(\"2_pipeline/tmp/\") # Location to save the models and logs\n",
    "outputPath=Path(projectDirectory/outputDirectory) # Create the full path to the output pictures\n",
    "\n",
    "# Define the design of experiments\n",
    "DOE = pd.DataFrame({\"dataset\" : []}) # Create a dataframe to run the script in batch mode\n",
    "#DOE = DOE.append({\"dataset\" : \"1\"},ignore_index=True)\n",
    "DOE = DOE.append({\"dataset\" : \"2\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"3\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"4\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"5\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"6\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"7\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"8\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"9\"},ignore_index=True)\n",
    "#DOE = DOE.append({\"dataset\" : \"10\"},ignore_index=True)\n",
    "\n",
    "## Training the model\n",
    "i = 0\n",
    "while i < DOE.count()[0]:\n",
    "    # File and folder locations\n",
    "    trainDataLabel = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_traindatalabel.pickle\")),\"rb\"))\n",
    "    valDataLabel = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_valdatalabel.pickle\")),\"rb\"))\n",
    "    \n",
    "    trainDataFeatures = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_traindatafeatures.pickle\")),\"rb\"))\n",
    "    valDataFeatures = pickle.load(open(Path(inputPath / os.path.join(DOE.iloc[i][\"dataset\"][0] + \"_valdatafeatures.pickle\")),\"rb\"))\n",
    "\n",
    "    ## Build the model: \n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st layer\n",
    "    model.add(Conv2D(filters=96, \n",
    "                     input_shape=(IMG_SIZE,IMG_SIZE,3), \n",
    "                     kernel_size=(11,11), \n",
    "                     strides=(4,4), \n",
    "                     padding=\"valid\", \n",
    "                     data_format=\"channels_last\", \n",
    "                     name='conv1'\n",
    "                    )\n",
    "             )\n",
    "    model.add(Activation('relu', name='activ1'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\", data_format=\"channels_last\", name='pool1'))\n",
    "    model.add(BatchNormalization(name='normal1'))\n",
    "\n",
    "    # 2nd layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv2'))\n",
    "    model.add(Activation('relu', name='activ2'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding=\"valid\", data_format=\"channels_last\", name='pool2'))\n",
    "    model.add(BatchNormalization(name='normal2'))\n",
    "\n",
    "    # 3rd layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\"valid\", data_format=\"channels_last\", name='conv3'))\n",
    "    model.add(Activation('relu', name='activ3'))\n",
    "    model.add(BatchNormalization(name='normal3'))\n",
    "\n",
    "    # Passing it to a dense layer\n",
    "    model.add(Flatten(name='flat1'))\n",
    "\n",
    "    # 6th layer\n",
    "    model.add(Dense(4096, \n",
    "                    input_shape=(IMG_SIZE*IMG_SIZE*3,), \n",
    "                    name='dense1'\n",
    "                   )\n",
    "             )\n",
    "    \n",
    "    model.add(Activation('relu', name='activ6'))\n",
    "    model.add(Dropout(0.5, name='drop1')) #to prevent overfitting\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 6th layer\n",
    "    model.add(Dense(4096, name='dense2'))\n",
    "    model.add(Activation('relu', name='activ7'))\n",
    "    model.add(Dropout(0.5, name='drop2')) #to prevent overfitting\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 7th layer\n",
    "    model.add(Dense(4096, name='dense3'))\n",
    "    model.add(Activation('relu', name='activ8'))\n",
    "    model.add(Dropout(0.5, name='drop3'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(47, name='dense4'))\n",
    "    model.add(Activation('softmax', name='activ9'))\n",
    "\n",
    "    #for i, layer in enumerate(model.layers): #To rename all layers\n",
    "    #    layer.name = 'layer_' + str(i)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(  \n",
    "        loss='sparse_categorical_crossentropy', # can be 'binary_crossentropy','sparse_categorical_crossentropy','categorical_crossentropy'\n",
    "        optimizer='sgd', # can be'sgd' or 'adam'\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.run_eagerly = False # To allow the use of callbacks tensorboard\n",
    "\n",
    "    date=str(datetime.now().strftime(\"%Y%m%d_%H%M%S\")) # Generate a unique name based on date and time to save the generated files \n",
    "    #savePath=Path(outputPath / date)\n",
    "    #savePath.mkdir\n",
    "    #modeldir=Path(MODEL / date)\n",
    "\n",
    "    logpath=Path(outputPath / date)\n",
    "    logpath.mkdir()\n",
    "\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=logpath)\n",
    "    \n",
    "    hist=model.fit(trainDataFeatures, trainDataLabel, \n",
    "                   callbacks=[tensorboard],\n",
    "              batch_size=64, # Number of images considered for each epoch\n",
    "              epochs=500, # Number of runs to train the model\n",
    "              verbose=1, #Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "              #validation_split=0.1,\n",
    "              validation_data=(valDataFeatures,valDataLabel), # Ratio trained/tested data\n",
    "              shuffle=False, #whether to shuffle the training data before each epoch\n",
    "             )\n",
    "    #modeldir.mkdir()\n",
    "    modelname=os.path.join(date + \"_exp2_BS64_3layers.model\")\n",
    "    model.save(Path(outputPath / modelname))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters to specify\n",
    "\n",
    "projectDirectory=Path(\"C:/Users/AQ62270/201910_ccn/NIPS/\") # Project path\n",
    "inputDirectory=Path(\"2_pipeline/store/\") # Location of the model\n",
    "modelName=Path(\"20200507_004956_exp2_BS64_3layers.model\") # Name of the model\n",
    "\n",
    "outputDirectory=Path(\"2_pipeline/store/\") # Location to save the figures\n",
    "storeDirectory=Path(\"2_pipeline/store/\") # Location to save the intermediate dataframes\n",
    "figPath = \"C:/Users/AQ62270/201910_ccn/NIPS/2_pipeline/store/\" # Location of saved figures for the paper\n",
    "\n",
    "inputPath=Path(projectDirectory/inputDirectory) # Create the full path to the input pictures\n",
    "outputPath=Path(projectDirectory/outputDirectory) # Create the full path to the output pictures\n",
    "storePath=Path(projectDirectory/storeDirectory) # Create the full path to the output pictures\n",
    "\n",
    "intermediateResults=True\n",
    "\n",
    "# Define design\n",
    "## Font size\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "## Figure dimensions\n",
    "pageWidth=174/25.4\n",
    "columnWidth=84/25.4\n",
    "baseLineNumbering=(0.5,-0.28) # Position of the captions under the sub-figures (letters or numbers)\n",
    "\n",
    "## Colors\n",
    "colorValSplit10=\"#d8b365\"\n",
    "colorValSplit30=\"#5ab4ac\"\n",
    "markerSplit10=\"o\"\n",
    "markerSplit30=\"s\"\n",
    "colorDefect=\"#f1a340\"\n",
    "colorNodefect=\"#998ec3\"\n",
    "# Open the model\n",
    "modelPath=os.path.join(inputPath,modelName)\n",
    "model = keras.models.load_model(\"C:/Users/AQ62270/201910_ccn/NIPS/2_pipeline/store/20200507_004956_exp2_BS64_3layers.model\")\n",
    "model.summary()\n",
    "# TO DO - plot the learning curve \n",
    "#plt.plot(hist.history['val_accuracy'])\n",
    "\n",
    "# Get the convolution layers adresses in the model\n",
    "convLayers=pd.DataFrame({\"layerIndex\":[], \"convolutionIndex\":[], \"layerName\":[]}, dtype=int)\n",
    "i=0\n",
    "j=1\n",
    "for layer in model.layers:\n",
    "    if \"conv\" in model.layers[i].name:\n",
    "        convLayers=convLayers.append({\"layerIndex\":i, \"convolutionIndex\":j, \"layerName\":model.layers[i].name}, ignore_index=True)\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "#layerNumber=[0,4,8,11,14] # Manually select the adress of convolution layers\n",
    "# TO DO - Do it automatically\n",
    "# layerNumber=convLayer.loc[].count\n",
    "\n",
    "# List the categories (If not done previously)\n",
    "lines = []\n",
    "buffer=[]\n",
    "with open(os.path.join(Path('C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/labels/test'+DS.iloc[0][\"dataset\"]+'.txt'))) as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    for image in lines:\n",
    "        buffer.append(os.path.split(image)[0])\n",
    "        categories = list( dict.fromkeys(buffer) )\n",
    "\n",
    "if intermediateResults==True:\n",
    "    print(convLayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save maximal value of the convolutional images for each convolution layer\n",
    "\n",
    "dfFeatureMap = pd.DataFrame({\"Layer\":[], \"filtre\" : [], \"image\":[], \"category\":[], \"maxima\":[]}, dtype=int)      \n",
    "\n",
    "# Create \"n\" submodels for \"n\" existing convolution layers\n",
    "for layer in convLayers[\"convolutionIndex\"]:\n",
    "    print(\"Layer number \", layer)\n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    modelLayer = Model(inputs=model.inputs, outputs=model.layers[layerNumber[layer-1]].output) # Create the submodel\n",
    "    \n",
    "    for category in tqdm(categories):  # For each folder, create a number for each category\n",
    "        class_num = categories.index(category) \n",
    "        for line in lines:\n",
    "            trueCategory = os.path.split(line)[0]\n",
    "            if category == trueCategory:\n",
    "                try:\n",
    "                    imageName=os.path.split(line)[1]\n",
    "                    img_array = cv2.imread(os.path.join(Path(dtd,category,os.path.split(line)[1])))  # open the image in grayscale\n",
    "                    img_array= cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    img_array = expand_dims(img_array, axis=0)                    \n",
    "                    for filterNumber in range(numberOfFilter):\n",
    "                        feature_maps = modelLayer.predict(img_array)\n",
    "                        dfFeatureMap=dfFeatureMap.append({\"Layer\": layer,\"filtre\" : filterNumber, \"image\":imageName, \"category\":category, \"maxima\":np.max(feature_maps[0, :, :, filterNumber])}, ignore_index=True)\n",
    "                except Exception as e:  # To keep the output clean\n",
    "                    pass\n",
    "    dfFeatureMap.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")),index=True)\n",
    "print(dfFeatureMap)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimized value mutual information threshold for each filter\n",
    "\n",
    "binsValue=30\n",
    "\n",
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "iconfusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[],\"category\":[], \"iThreshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"iInfoMutuelle\":[]}, dtype=int)\n",
    "\n",
    "for layer in convLayers[\"convolutionIndex\"]:\n",
    "    print(\"Layer number : \", layer)    \n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    \n",
    "    for filterNumber in range (numberOfFilter):\n",
    "        print(\"filter number : \",filterNumber)\n",
    "        for category in categories:\n",
    "            print(\"category : \",category)\n",
    "            #dfinodefect=dfFeatureMap[(dfFeatureMap['category']==\"nodefect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            #dfidefect=dfFeatureMap[(dfFeatureMap['category']==\"defect\") & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)]\n",
    "            minND=np.amin(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            maxND=np.amax(dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber)][\"maxima\"])\n",
    "            thresholds=np.arange(minND, maxND, (maxND-minND)/20)\n",
    "            for iThreshold in (thresholds):\n",
    "                iTP=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iTN=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iFP=dfFeatureMap[(dfFeatureMap['category']!=category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=iThreshold)].count()[4]\n",
    "                iFN=dfFeatureMap[(dfFeatureMap['category']==category) & (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<iThreshold)].count()[4]\n",
    "                iS=iTP+iTN+iFP+iFN\n",
    "                label_true=[*np.ones(iTP),*np.zeros(iTN),*np.zeros(iFP),*np.ones(iFN)]\n",
    "                label_pred=[*np.ones(iTP),*np.zeros(iTN),*np.ones(iFP),*np.zeros(iFN)]\n",
    "                \n",
    "                imutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "                iPY1 = (iTP+iFP)/iS\n",
    "                iPY0 = (iFN+iTN)/iS\n",
    "                iPC1 = (iTP+iFN)/iS\n",
    "                iPC0 = (iFP+iTN)/iS\n",
    "                iPY1C1, iPY0C1, iPY1C0, iPY0C0 = iTP/iS, iFN/iS, iFP/iS, iTN/iS\n",
    "                \n",
    "                if (iFP!=0 and iFN!=0 and iTP!=0 and iTN!=0):\n",
    "                    iHY = - iPY1*math.log2(iPY1) - iPY0*math.log2(iPY0)\n",
    "                    iHC = - iPC1*math.log2(iPC1) - iPC0*math.log2(iPC0)\n",
    "                    iHYC = - iPY1C1*math.log2(iPY1C1) - iPY0C1*math.log2(iPY0C1) - iPY1C0*math.log2(iPY1C0) - iPY0C0*math.log2(iPY0C0)\n",
    "                    iIYC = iHY + iHC - iHYC\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":imutualScore, \"iInfoMutuelle\":iIYC}, ignore_index=True)        \n",
    "                else :\n",
    "                    iconfusionMatrix=iconfusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"iThreshold\":iThreshold, \"truePositive\":iTP,\"trueNegative\":iTN, \"falsePositive\":iFP, \"falsenegative\":iFN, \"mutualScore\":np.NaN, \"iInfoMutuelle\":np.NaN}, ignore_index=True)\n",
    "    iconfusionMatrix.to_csv(Path(os.path.join(outputPath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix.csv\")),index=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built the confusion matrix with optimized value of mutual information for each filter\n",
    "iconfusionMatrix = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_iconfusionMatrix.csv\")))\n",
    "dfFeatureMap = pd.read_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_dfFeatureMap.csv\")))\n",
    "confusionMatrix=pd.DataFrame({\"Layer\":[], \"filter\":[], \"category\":[], \"threshold\":[], \"truePositive\":[],\"trueNegative\":[], \"falsePositive\":[], \"falsenegative\":[], \"mutualScore\":[], \"mutualInfo\":[]}, dtype=int)\n",
    "layerNumber=[0,4,8]\n",
    "for layer in convLayers[\"convolutionIndex\"]:\n",
    "    numberOfFilter = len(model.layers[layerNumber[layer-1]].get_weights()[1])\n",
    "    print(layer)\n",
    "    for filterNumber in range (numberOfFilter):\n",
    "        print(filterNumber)\n",
    "        for category in categories:\n",
    "            threshold=iconfusionMatrix.loc[(iconfusionMatrix[\"category\"]==category)&(iconfusionMatrix[\"Layer\"]==layer)&(iconfusionMatrix[\"filter\"]==filterNumber)].sort_values(by=\"iInfoMutuelle\", ascending=False).iloc[0][\"iThreshold\"]\n",
    "            \n",
    "            TP=dfFeatureMap[(dfFeatureMap['category']==category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=threshold)].count()[4]\n",
    "            TN=dfFeatureMap[(dfFeatureMap['category']!=category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<threshold)].count()[4]\n",
    "            FP=dfFeatureMap[(dfFeatureMap['category']!=category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]>=threshold)].count()[4]\n",
    "            FN=dfFeatureMap[(dfFeatureMap['category']==category) &  (dfFeatureMap['Layer']==layer) & (dfFeatureMap['filtre']==filterNumber) & (dfFeatureMap[\"maxima\"]<threshold)].count()[4]\n",
    "            S=TP+TN+FP+FN\n",
    "            \n",
    "            # Calcul of the mutual information\n",
    "            PY1 = (TP+FP)/S\n",
    "            PY0 = (FN+TN)/S\n",
    "            PC1 = (TP+FN)/S\n",
    "            PC0 = (FP+TN)/S \n",
    "            PY1C1, PY0C1, PY1C0, PY0C0 = TP/S, FN/S, FP/S, TN/S\n",
    "            \n",
    "            HY = - PY1*math.log2(PY1) - PY0*math.log2(PY0)\n",
    "            HC = - PC1*math.log2(PC1) - PC0*math.log2(PC0)\n",
    "            HYC = - PY1C1*math.log2(PY1C1) - PY0C1*math.log2(PY0C1) - PY1C0*math.log2(PY1C0) - PY0C0*math.log2(PY0C0)     \n",
    "            IYC = HY + HC - HYC\n",
    "        \n",
    "            label_true=[*np.ones(TP),*np.zeros(TN),*np.zeros(FP),*np.ones(FN)]\n",
    "            label_pred=[*np.ones(TP),*np.zeros(TN),*np.ones(FP),*np.zeros(FN)]\n",
    "            mutualScore=normalized_mutual_info_score(label_true, label_pred)\n",
    "    \n",
    "            confusionMatrix=confusionMatrix.append({\"Layer\":layer, \"filter\":filterNumber,\"category\":category, \"threshold\":threshold, \"truePositive\":TP,\"trueNegative\":TN, \"falsePositive\":FP, \"falsenegative\":FN, \"mutualScore\":mutualScore, \"mutualInfo\":IYC}, ignore_index=True)\n",
    "\n",
    "confusionMatrix.to_csv(Path(os.path.join(storePath,os.path.splitext(modelName)[0]+\"_confusionMatrix.csv\")),index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mutual information (last decile) through the Newkork (sorted) - Pandas version - DRAFT\n",
    "\n",
    "#confusionMatrix = pd.read_csv(\"/home/quentin/Documents/NIPS/2_pipeline/store/20200405_162613_exp2_BS64_confusionMatrix.csv\")\n",
    "layerToSort=0\n",
    "viridis = cm.get_cmap('viridis', 12)\n",
    "fig, ax = plt.subplots(1,1,figsize=(pageWidth,7))\n",
    "i=0.1\n",
    "layers = [1,2,3,4,5]\n",
    "tab=pd.DataFrame({\"layer\":[],\"category\":[], \"decile\":[], \"category\":[]})\n",
    "SMALL_SIZE=9\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "for layer in layers:\n",
    "    for category in categories:    \n",
    "        tab=tab.append({\"layer\":layer, \"category\":category, \"decile\":np.percentile(confusionMatrix[(confusionMatrix[\"Layer\"]==layer)&(confusionMatrix[\"category\"]==category)][\"mutualInfo\"],np.arange(0,100,10))[9], \"category\":category},ignore_index=True)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for layer in (layers):\n",
    "    xDecile=np.array(tab[tab[\"layer\"]==layer][\"decile\"])\n",
    "    yCategory=np.array(tab[tab[\"layer\"]==layer][\"category\"])\n",
    "    x=np.append(x,xDecile,axis=0)\n",
    "    y=np.append(y,yCategory,axis=0)\n",
    "x=x.reshape(5,47)\n",
    "y=y.reshape(5,47) #test\n",
    "g=x[:,np.argsort(x[0])]\n",
    "sortedCategory=y[0,np.argsort(x[layerToSort])] #test\n",
    "im = ax.imshow(g)\n",
    "ax.set_yticks(np.arange(0,len(layers)))\n",
    "ax.set_yticklabels(layers)\n",
    "ax.set_xticks(np.arange(len(categories)))\n",
    "ax.set_xticklabels(sortedCategory)\n",
    "ax.set_ylabel('Layers')\n",
    "ax.set_title(\"Mutual information score through the network (last decile)\")\n",
    "ax.vlines(np.arange(-0.5,len(categories)-1), -.5, len(layers)-.5, 'w')\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, va=\"center\", ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "cbar=fig.colorbar(im,ax=ax, pad=0.01, aspect=5, ticks=[0,0.01,0.02], shrink=0.22)\n",
    "cbar.set_ticks([0,0.01,0.02])\n",
    "cbar.set_ticklabels([0,0.01,0.02])\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"/home/quentin/Documents/NIPS/2_pipeline/store/20200405_162613_exp2_BS64_MILastDecile.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory=\"C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/images/spiralled/\"\n",
    "confusionMatrix = pd.read_csv(Path(os.path.join(\"C:/Users/AQ62270/201910_ccn/NIPS/2_pipeline/store/20200405_162613_exp2_BS64_confusionMatrix.csv\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#modelName=\"20200405_162613_exp2_BS64.model\"\n",
    "inputDirectory=Path(\"2_pipeline/store/\") # Location of the model\n",
    "inputPath=Path(projectDirectory/inputDirectory)\n",
    "modelPath=os.path.join(inputPath,modelName)\n",
    "model_5layers = keras.models.load_model(os.path.join(Path(projectDirectory/inputDirectory),\"20200405_162613_exp2_BS64.model\"))\n",
    "model_3layers = keras.models.load_model(os.path.join(Path(projectDirectory/inputDirectory),\"20200507_004956_exp2_BS64_3layers.model\"))\n",
    "\n",
    "model_5layers_1stConv=Model(inputs=model_5layers.inputs, outputs=model_5layers.layers[layerNumber[layer-1]].output)\n",
    "model_3layers_1stConv=Model(inputs=model_3layers.inputs, outputs=model_3layers.layers[layerNumber[layer-1]].output)\n",
    "\n",
    "model_5layers_1stConv.summary()\n",
    "model_3layers_1stConv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array=cv2.imread(os.path.join(Path(\"C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/images/spiralled/spiralled_0004.jpg\")))\n",
    "img_array = cv2.resize(img_array, (227, 227))  # resize the image\n",
    "plt.imshow(new_array[:,:,0])\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur11 = cv2.GaussianBlur(feature_maps[0, :, :,filterNumber],(11,11),0)\n",
    "blur7 = cv2.GaussianBlur(feature_maps[0, :, :,filterNumber],(7,7),0)\n",
    "DoGimg = blur10 - blur7\n",
    "plt.imshow(blur11)\n",
    "plt.show()\n",
    "plt.imshow(blur7)\n",
    "plt.show()\n",
    "plt.imshow(DoGimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filter or associated feature maps / model CNN2\n",
    "layer=1\n",
    "\n",
    "img_array=cv2.imread(os.path.join(Path(\"C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/images/spiralled/spiralled_0004.jpg\")))\n",
    "img_array = cv2.resize(img_array, (227, 227))  # resize the image\n",
    "img_array = expand_dims(img_array, axis=0)\n",
    "plt.imshow(new_array)\n",
    "plt.show()\n",
    "\n",
    "cols=8\n",
    "rows=12\n",
    "fig,ax = plt.subplots(nrows=rows, ncols=cols, figsize=(15,20))\n",
    "filterNumber=0\n",
    "for i in np.arange(0,cols):\n",
    "    for j in np.arange(0,rows):\n",
    "        # plot feature maps or...\n",
    "        #feature_maps = model_5layers_1stConv.predict(img_array)\n",
    "        #ax[j,i].imshow(feature_maps[0, :, :,filterNumber], cmap='gray')\n",
    "        \n",
    "        # ...plot filters (comment last paragraph)\n",
    "        ax[j,i].imshow(model_5layers_1stConv.layers[layerNumber[layer-1]+1].get_weights()[0][:,:,0, filterNumber], cmap='gray')\n",
    "        \n",
    "        filterNumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filter or associated feature maps / model CNN3\n",
    "\n",
    "cols=8\n",
    "rows=12\n",
    "fig,ax = plt.subplots(nrows=rows, ncols=cols, figsize=(15,20))\n",
    "filterNumber=0\n",
    "for i in np.arange(0,cols):\n",
    "    for j in np.arange(0,rows):\n",
    "        # plot feature maps or...\n",
    "        #feature_maps = model_3layers_1stConv.predict(img_array)\n",
    "        #ax[j,i].imshow(feature_maps[0, :, :,filterNumber], cmap='gray')\n",
    "        \n",
    "        # ...plot filters (comment last paragraph)\n",
    "        ax[j,i].imshow(model_3layers_1stConv.layers[layerNumber[layer-1]+1].get_weights()[0][:,:,0, filterNumber], cmap='gray')\n",
    "        \n",
    "        filterNumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps.shape\n",
    "print(feature_maps[0,0,1,:])\n",
    "\n",
    "for i in  (np.arange(0,55))[:2]:\n",
    "    for j in  (np.arange(0,55))[:2]:\n",
    "        for filterNumber in np.arange(0,96):\n",
    "            #print(feature_maps[0,i,j,:])\n",
    "            blur11 = cv2.GaussianBlur(feature_maps[0, j, i,filterNumber],(11,11),0)\n",
    "            blur7 = cv2.GaussianBlur(feature_maps[0, j, i,filterNumber],(7,7),0)\n",
    "            DoGimg = blur10 - blur7\n",
    "            ax[j,i].imshow(DoGimg)\n",
    "plt.show()\n",
    "#        feature_maps = model_3layers_1stConv.predict(img_array)\n",
    "#        plt.imshow(feature_maps[0, :, :,filterNumber], cmap='gray')\n",
    "#        filterNumber+=1\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps[0, :, :,filterNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "blur11 = cv2.GaussianBlur(feature_maps[0, :, :,filterNumber],(11,11),0)\n",
    "blur7 = cv2.GaussianBlur(feature_maps[0, :, :,filterNumber],(7,7),0)\n",
    "DoGimg = blur10 - blur7\n",
    "plt.imshow(blur11)\n",
    "plt.show()\n",
    "plt.imshow(blur7)\n",
    "plt.show()\n",
    "plt.imshow(DoGimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix[(confusionMatrix[\"Layer\"]==1)].sort_values(by=\"mutualInfo\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array=cv2.imread(os.path.join(Path(\"C:/Users/AQ62270/201910_ccn/NIPS/0_data/dtd/images/spiralled/spiralled_0004.jpg\")))\n",
    "new_array = cv2.resize(img_array, (227, 227))  # resize the image\n",
    "plt.imshow(new_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredImage = cv2.GaussianBlur(,(5,5),0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
